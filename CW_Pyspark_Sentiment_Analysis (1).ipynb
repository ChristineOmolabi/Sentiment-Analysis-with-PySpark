{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw5HslcmNW5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9cd074-f1f0-4a57-ad29-53f31214c4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-19 10:25:16--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2023-08-19 10:25:16--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‚ÄòSTDOUT‚Äô\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-19 10:25:16 (88.3 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.0.2\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.0.2\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m502.1/502.1 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#Installing java, apache spark, hadoop and findspark\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.2.3 -s 5.0.2\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install spark-nlp==5.0.2\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbzVBVXkOfkE",
        "outputId": "ae7c885a-91de-4edd-d03b-f5a770ff4c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp==5.0.2 in /usr/local/lib/python3.10/dist-packages (5.0.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.2.3)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "d7vMCXPgOqGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Begin session\n",
        "import sparknlp #library for sentiment analysis from Jonsnowlabs\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "bQ6Lcf8aOuoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "data = spark.read.csv('tokyo_2020_tweets.csv', header = True, inferSchema = True)"
      ],
      "metadata": {
        "id": "C_NKWTu7PHFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select corpus column from dataset\n",
        "df = data.select('text')\n",
        "df = df.dropna()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkIMutC2P24M",
        "outputId": "6c7a9c66-1cc7-47b9-bbb5-be997f78eec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "| Let the party begin|\n",
            "|Congratulations #...|\n",
            "|   Big Breaking Now |\n",
            "|     Q4: üá¨üáß3-1üáøüá¶|\n",
            "|All I can think o...|\n",
            "|#Tokyo2020 #Olympics|\n",
            "|Can't help but ch...|\n",
            "|@inquirerdotnet @...|\n",
            "|    Q3 üá®üá¶ 1-4 üá©üá™|\n",
            "|Hearty Congratula...|\n",
            "|                 0.0|\n",
            "|Gymnastics ‚ù§Ô∏è #To...|\n",
            "|Morning everyone!...|\n",
            "| #Tokyo2020 #Tennis |\n",
            "|Up next for Carlo...|\n",
            "|Congrates @miraba...|\n",
            "|The wait for a we...|\n",
            "|#Tokyo2020   #Mir...|\n",
            "|#Tokyo2020 #Olymp...|\n",
            "|Well done to #Tea...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the corpus by removing any symbols\n",
        "from pyspark.sql.functions import udf, col, lower, regexp_replace #switchup the order of stuff\n",
        "# Clean corpus\n",
        "df_clean = df.select((lower(regexp_replace('text', \"[^a-zA-Z\\\\s]\", \"\")).alias('cleaned_text')))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAw5pr4FQDcS",
        "outputId": "39f2eac1-9bb4-4c42-b472-fa15f8e515ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "| Let the party begin|\n",
            "|Congratulations #...|\n",
            "|   Big Breaking Now |\n",
            "|     Q4: üá¨üáß3-1üáøüá¶|\n",
            "|All I can think o...|\n",
            "|#Tokyo2020 #Olympics|\n",
            "|Can't help but ch...|\n",
            "|@inquirerdotnet @...|\n",
            "|    Q3 üá®üá¶ 1-4 üá©üá™|\n",
            "|Hearty Congratula...|\n",
            "|                 0.0|\n",
            "|Gymnastics ‚ù§Ô∏è #To...|\n",
            "|Morning everyone!...|\n",
            "| #Tokyo2020 #Tennis |\n",
            "|Up next for Carlo...|\n",
            "|Congrates @miraba...|\n",
            "|The wait for a we...|\n",
            "|#Tokyo2020   #Mir...|\n",
            "|#Tokyo2020 #Olymp...|\n",
            "|Well done to #Tea...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules and classes\n",
        "from sparknlp.base import DocumentAssembler, Pipeline, Finisher\n",
        "from sparknlp.annotator import (SentenceDetector,Tokenizer,Lemmatizer,SentimentDetector)\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.common import *"
      ],
      "metadata": {
        "id": "qI7SpUEjQIVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the sentiment and lemmatization dictionaries from the web\n",
        "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp\n",
        "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P /tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeOFwFPIRpCH",
        "outputId": "fbeeb953-0a2a-4c72-aec8-78d224687453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-19 10:30:21--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.232.24, 52.216.53.216, 54.231.197.240, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.232.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189437 (185K) [text/plain]\n",
            "Saving to: ‚Äò/tmp/lemmas_small.txt‚Äô\n",
            "\n",
            "\rlemmas_small.txt      0%[                    ]       0  --.-KB/s               \rlemmas_small.txt    100%[===================>] 185.00K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-08-19 10:30:22 (2.12 MB/s) - ‚Äò/tmp/lemmas_small.txt‚Äô saved [189437/189437]\n",
            "\n",
            "--2023-08-19 10:30:22--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.85.110, 52.216.43.32, 52.217.225.120, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.85.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 289 [text/plain]\n",
            "Saving to: ‚Äò/tmp/default-sentiment-dict.txt‚Äô\n",
            "\n",
            "default-sentiment-d 100%[===================>]     289  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-19 10:30:22 (5.42 MB/s) - ‚Äò/tmp/default-sentiment-dict.txt‚Äô saved [289/289]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms corpus to document annotation for pipeline model\n",
        "assembler = (DocumentAssembler().setInputCol(\"cleaned_text\").setOutputCol(\"assembled_text\"))"
      ],
      "metadata": {
        "id": "3fvfw4zCQNfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Detection for pipeline model\n",
        "sentence_detect = SentenceDetector().setInputCols([\"assembled_text\"]).setOutputCol(\"detected_sent\")"
      ],
      "metadata": {
        "id": "hu2hOyclQTdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "tokenizer = Tokenizer().setInputCols([\"detected_sent\"]).setOutputCol(\"tokenized\")"
      ],
      "metadata": {
        "id": "Tat8iBcCQWau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "lemma = Lemmatizer().setInputCols(\"tokenized\").setOutputCol(\"lemmatized_text\").setDictionary(\"/tmp/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "oJdfVR9sQZDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment Detection\n",
        "sentiment_detector= (SentimentDetector().setInputCols([\"lemmatized_text\", \"detected_sent\"])\n",
        "    .setOutputCol(\"result\")\n",
        "    .setDictionary(\"/tmp/default-sentiment-dict.txt\", \",\"))"
      ],
      "metadata": {
        "id": "j0Uj9fs7QclH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finisher\n",
        "finit= (Finisher().setInputCols([\"result\"]).setOutputCols(\"sentiments\"))"
      ],
      "metadata": {
        "id": "iEm2Jho9QiXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline\n",
        "pipes = Pipeline(stages=[assembler,sentence_detect,tokenizer,lemma,sentiment_detector,finit])"
      ],
      "metadata": {
        "id": "mYynVzaWQmhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit-transform to get the sentiment predictions\n",
        "predictor = pipes.fit(df_clean).transform(df_clean).collect()"
      ],
      "metadata": {
        "id": "O15EAPMTQpMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "stuvvs = spark.createDataFrame(predictor).show(truncate = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBzkh9drWOQe",
        "outputId": "bc310834-ef5c-4bc2-ed87-615702f74876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
            "|                                                                                                             cleaned_text|sentiments|\n",
            "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
            "|                                                                                                      let the party begin|[positive]|\n",
            "|                                                                                   congratulations tokyo httpstcoofkmsukq|[positive]|\n",
            "|                                                                                                        big breaking now |[positive]|\n",
            "|                                                                                                                       q |[positive]|\n",
            "|  all i can think of every time i watch the rings event tokyo artisticgymnastics olympics olympicgames httpstcocjaxefnyzd|[positive]|\n",
            "|                                                                                                           tokyo olympics|[positive]|\n",
            "|                                                                     cant help but cheer for them banda  goals in  games |[positive]|\n",
            "|      inquirerdotnet ftjochoainq caloy yulos  however was only good for sixth in the rings preliminaries httpstcobxpmerbf|[positive]|\n",
            "|                                                                                                                     q   |[positive]|\n",
            "|                                                                                               hearty congratulations to |[positive]|\n",
            "|                                                                                                                         |[positive]|\n",
            "|                                                                                                        gymnastics  tokyo|[positive]|\n",
            "|                                morning everyone caffeine check lorilindsey lroman tokyo tokyoolympics httpstcoghtxfkqddx|[positive]|\n",
            "|                                                                                                            tokyo tennis |[positive]|\n",
            "|                                                 up next for carlos yulo is vault artisticgymnastics tokyo mbsportsonline|[positive]|\n",
            "|                                                               congrates mirabaichanu you make our country proudas always|[positive]|\n",
            "|                                                                              the wait for a weightlifting medal is over |[positive]|\n",
            "|tokyo   mirabaichanu  weightlifting  wait of  years ends for india well done mirabaichanu  do i need to httpstcowcsrftyek|[positive]|\n",
            "|                                                               tokyo olympics a very proud moment for all of use jai hind|[positive]|\n",
            "|                                                                             well done to teamgb in the hockey good game |[positive]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}